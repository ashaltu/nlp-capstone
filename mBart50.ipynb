{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf sample_data/\n",
    "!pip install OpenNMT-tf\n",
    "!pip install gdown\n",
    "!pip install sacremoses\n",
    "!pip install transformers\n",
    "!pip install sentencepiece\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "import opennmt\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "import sacrebleu\n",
    "import pyonmttok\n",
    "from sacremoses import MosesDetokenizer\n",
    "from opennmt.utils import checkpoint as checkpoint_util\n",
    "from pyonmttok import SentencePieceTokenizer\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive',force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, train_size, val_size, test_size):\n",
    "  if train_size + val_size + test_size != 1.0:\n",
    "    raise Exception(\"Train, validation, and test sizes must add up to 1.\") \n",
    "  \n",
    "  train_mark = int(len(data) * train_size)\n",
    "  val_mark = train_mark + int(len(data) * val_size)\n",
    "\n",
    "  train_data = data[0:train_mark]\n",
    "  val_data = data[train_mark:val_mark]\n",
    "  test_data = data[val_mark:]\n",
    "\n",
    "  return train_data, val_data, test_data\n",
    "\n",
    "def save_data(data, data_folder_name, filename):\n",
    "  with open(os.path.join(data_folder_name, filename), mode=\"w\") as f:\n",
    "    for line in data:\n",
    "      if line.strip():\n",
    "        f.write(line)\n",
    "\n",
    "def count_weights(model):\n",
    "  trainable_count = np.sum([K.count_params(w) for w in model.trainable_weights])\n",
    "  non_trainable_count = np.sum([K.count_params(w) for w in model.non_trainable_weights])\n",
    "\n",
    "  print('Total params: {:,}'.format(trainable_count + non_trainable_count))\n",
    "  print('Trainable params: {:,}'.format(trainable_count))\n",
    "  print('Non-trainable params: {:,}'.format(non_trainable_count))\n",
    "\n",
    "def display_weights(model):\n",
    "  for layer in model.encoder.layers:\n",
    "    print(f\"===== LAYER: {layer.name} =====\")\n",
    "    if layer.get_weights() != []:\n",
    "        weights = layer.get_weights()[0]\n",
    "        biases = layer.get_weights()[1]\n",
    "        print(\"weights:\")\n",
    "        print(weights)\n",
    "        print(\"biases:\")\n",
    "        print(biases)\n",
    "    else:\n",
    "        print(\"weights: \", [])\n",
    "\n",
    "def compute_scores(runner, features_filename, labels_filename, pred_filename, include_ppl=False, include_ter=False):\n",
    "  runner.infer(features_filename, pred_filename)\n",
    "\n",
    "  dot_idx = pred_filename.index('.')\n",
    "  base_pred_name = pred_filename[0:dot_idx]\n",
    "  dot_idx = labels_filename.index('.')\n",
    "  base_model_name = labels_filename[0:dot_idx]\n",
    "  pred_filename = detokenize_data(base_pred_name, base_model_name)\n",
    "  detokenized_labels_filename = detokenize_data(base_model_name, base_model_name)\n",
    "  preds = []\n",
    "  truth = []\n",
    "  with open(pred_filename) as f:\n",
    "    preds = f.readlines()\n",
    "\n",
    "  with open(detokenized_labels_filename) as f:\n",
    "    truth = f.readlines()\n",
    "\n",
    "  scores = dict()\n",
    "  if include_ppl:\n",
    "    scores = runner.evaluate(\n",
    "        features_file=features_filename,\n",
    "        labels_file=labels_filename)\n",
    "  \n",
    "  bleu = sacrebleu.corpus_bleu(preds, [truth])\n",
    "  scores.update({'bleu': bleu.score})\n",
    "  if include_ter:\n",
    "    ter = sacrebleu.corpus_ter(preds, [truth])\n",
    "    scores.update({'ter': ter.score})\n",
    "  \n",
    "  return scores\n",
    "\n",
    "def tokenize_data(save_folder_name, basename):\n",
    "  tokenize_sub_data(save_folder_name, basename, \"train\")\n",
    "  tokenize_sub_data(save_folder_name, basename, \"test\")\n",
    "  tokenize_sub_data(save_folder_name, basename, \"val\")\n",
    "\n",
    "def tokenize_sub_data(save_folder_name, basename, set_type):\n",
    "  model_path = os.path.join(\"sentencepiece_models\", f\"{basename}.model\")\n",
    "  vocabulary_path = os.path.join(\"sentencepiece_models\", f\"{basename}.vocab\")\n",
    "  tokenizer = SentencePieceTokenizer(model_path=model_path,\n",
    "                                     vocabulary_path=vocabulary_path,)\n",
    "  \n",
    "  with open(os.path.join(f\"{save_folder_name}_raw\", f\"{basename}_{set_type}.raw\")) as f:\n",
    "    with open(os.path.join(save_folder_name, f\"{basename}_{set_type}.tok\"), mode=\"w\") as fout:\n",
    "      for line in f.readlines():\n",
    "        if line.strip():\n",
    "          fout.write(\" \".join(tokenizer.tokenize(line)[0]) + \"\\n\")\n",
    "\n",
    "\n",
    "def detokenize_data(tokenized_basename, model_basename):\n",
    "  model_path = os.path.join(\"sentencepiece_models\", model_basename + \".model\")\n",
    "  vocabulary_path = os.path.join(\"sentencepiece_models\", f\"{model_basename}.vocab\")\n",
    "  tokenizer = SentencePieceTokenizer(model_path=model_path,\n",
    "                                     vocabulary_path=vocabulary_path,)\n",
    "  \n",
    "  with open(f\"{tokenized_basename}.tok\") as f:\n",
    "    with open(f\"{tokenized_basename}.txt\", mode=\"w\") as fout:\n",
    "      for line in f.readlines():\n",
    "        fout.write(tokenizer.detokenize(line.strip().split(\" \")) + \"\\n\")\n",
    "\n",
    "  return f\"{tokenized_basename}.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\", use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_d = []\n",
    "\n",
    "with open(\"pred.txt\") as f:\n",
    "  src_d = f.readlines()\n",
    "  src_d = src_d[:1000]\n",
    "\n",
    "out = []\n",
    "tokenizer.src_lang = \"es_XX\"\n",
    "for l in src_d:\n",
    "  encoded_es = tokenizer(l, return_tensors=\"pt\",truncation=True,padding=True)\n",
    "  generated_tokens = model.generate(\n",
    "    **encoded_es,\n",
    "    forced_bos_token_id=tokenizer.lang_code_to_id[\"it_IT\"]\n",
    "  )\n",
    "  out.append(tokenizer.batch_decode(generated_tokens, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"final.out\", 'w') as fout:\n",
    "    for line in out:\n",
    "      fout.write(line[0] + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
